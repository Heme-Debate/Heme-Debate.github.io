---
title: "Project Two"
date: '2021-05-08'
output:
  html_document: default
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = TRUE, fig.align = "center", 
    warning = F, message = F, tidy = TRUE, tidy.opts = list(width.cutoff = 60), 
    R.options = list(max.print = 100))
```

Brandon Deeb (btd543)



And away we go! Loading stuff up. This is data was obtained from Kraggle and is essentially a breakdown of the Melbourne housing market, which has 'cooled down' recently. I had a few other ideas in mind before this one, such as pitching in professional baseball, but it turns out sometimes there can actually be *too much* data: something I had never considered. But, this dataset is really nice! There are 8887 houses measured in this dataset, which is absolutely monstrous to conceptualize. Some of the main variables of interest include number of rooms, 'type' of home (i.e. house, duplex, townhouse), price, number of bathrooms, amount of land included with property, year built, and a bunch of other really useful things. I removed some of the other undesirables, like seller information or date listed. All of the variables retained measure pretty much exactly what they sound like, and are really interesting to think about in regards to being a homeowner. As someone from South Texas, land size always seemed pretty generous; everywhere you look people have insanely big yards or open fields behind their house, so it's interesting to see what's of value for people looking to buy homes in Melbourne (a lot less garage space than South Texas). 

```{R}

library(tidyverse)
library(dplyr)
library(rstatix)
library(ggpubr)
library(sandwich)
library(lmtest)
library(RColorBrewer)
library(plotROC)
library(pROC)
getwd()
lol <- read.csv("Melbourne_housing_FULL.csv")
lol %>% na.omit() -> lol
lol %>% glimpse()
lol %>% dim()
lol %>% select(1,3,4,5,12,13,14,15,16,17,20) -> lol
lol %>% glimpse()

```

Now, we run a MANOVA test to compare the number of rooms to the other important numeric variables, such as Price, Landsize, and BuildingArea. I had to set the 'Rooms' variable to a categorical instead of a numeric value, and followed it up by double-checking the degrees of freedom (because the possibility of ten rooms...whattt??). 

```{R}

lol$Rooms <- as.character(as.numeric(lol$Rooms))
lol %>% glimpse()

manova <- manova(cbind(Price,Landsize,BuildingArea)~Rooms, data=lol)
summary(manova)

roomy <- unique(lol$Rooms)
roomy %>% glimpse


```

As expected, p-value is way below the cutoff, so there're interactions of some sort at play here. Running univariate ANOVA tests to see where. Again, unsurprisingly, there is some sort of interaction between the number of rooms a house has and its price, landsize, and building area. Not too unexpected here; the more rooms a house has, the more area it takes up. This kinda goes interchangeably with landsize, except landsize includes areas surrounding the house as well. This increase in building area more materials, so higher price. 

```{R}

summary.aov(manova)

```
Using a paired t-test to determine which groups differ. This yielded some pretty weird results where the entire t-test was empty due to there only being one entry for the 10 and 12 room houses, so I removed them. They were really gross outliers though, so it makes sense to remove them. After running all t-tests, I found that there weren't really enough observations in the 7 and 8 room houses either and they were really messing with the data. It suggested there wasn't really significant differences between a 1 room and an 8 room house, which was just blatantly wrong, so they were omitted. All of the t-tests have a similar trend: almost all of the numerical variables varied significantly by each number of rooms, with the interaction getting slightly lower between houses with 5 and 6 rooms. Logically, this all follows. Houses that big are changing by such small amounts that it's probably not that significant percentage wise (i.e. price goes up some thousands, but the total is in the millions, or area goes up another 100 square meters in a 2000+ sized house). This led to a really wild 49 total tests, and a corrected type I error of 0.05/49 or 0.00102. This means that there is no significant difference between price of 5 and 6 room houses, no significant difference between landsizes of 4, 5, and 6 room houses, and no significant difference between building area of 5 and 6 room houses.  All other interaction are significant! In regards to assumptions, this MANOVA likely breaks the multivariate normality assumption, homogeneity of covariance, assuming linearity among DVs (stuff in expensive homes typically isn't the same price/level of luxury as it is in small homes; a kitchen in a 2 room house is not the same as a kitchen in a 5 room house), no extreme outliers (some of these prices are outrageous), and *maybe* no collinearity. Overall, MANOVA with further analysis was good for a starting perception of things, but violates a lot of assumptions for us to take this at face value without further analysis. 

```{r}

lol$Price <- as.numeric(as.integer(lol$Price))

lol[lol$Rooms != "12",] -> lol
lol[lol$Rooms != "10",] -> lol
lol[lol$Rooms != "8",] -> lol
lol[lol$Rooms != "7",] -> lol

table(lol$Rooms)

pairwise.t.test(lol$Price, lol$Rooms, p.adj="none")
pairwise.t.test(lol$Landsize, lol$Rooms, p.adj="none")
pairwise.t.test(lol$BuildingArea, lol$Rooms, p.adj="none")

```

This linear regression model is aimed at seeing how price is affected by building area and number of rooms. First, I centered all the desired numerical values. The resulting regression model uses 1 room houses as the reference, showing a price of 932,128 Australian dollars of average building area. Things start to get a little weird here, though, with price actually decreasing for a 2 room house, and barely increasing for a 6 room house. The interaction between area and room number might help compensate for this, though. While the other room values increase greatly, their building area interaction decreases, meaning while the price might still go up, it's a sort of "balance" between the two. The only positive building area interaction is with homes with 6 rooms, with a drastic slope. This follows the aforementioned, as 6 room homes lose a bit of price in the rooms interaction, but gain a lot in the building area interaction. It can also be noted that these are centered values, so it is also likely that these lower values come in below the average, and are therefore negative! This means their contribution to the Price ends up being a positive one. The regression was plotted in order to visualize relationships. For houses of all room sizes, the slope is positive, showing the relationship between increased building area and an increased cost.
 
```{R}

mean(lol$Price)
lol$Price_c <- (lol$Price-mean(lol$Price))


mean(lol$BuildingArea)
lol$BuildingArea_c <- (lol$BuildingArea-mean(lol$BuildingArea))

fit <- lm(Price ~ BuildingArea_c * Rooms, data = lol)
fit %>% summary()

nb.cols<-6
mycolors <- colorRampPalette(brewer.pal(8, "YlOrRd"))(nb.cols)
ggplot(lol, aes(lol$BuildingArea, lol$Price, color=lol$Rooms))+ geom_point() + geom_smooth(method = "lm", se = F, fullrange = T) + geom_vline(xintercept=mean(lol$BuildingArea)) + coord_cartesian(xlim=c(0,1000)) + xlab("Building Area") + ylab("Price") + scale_color_manual(values=mycolors)

```
 
The amount of variance in Price we can explain with this model is only an adjusted 0.315.  In testing the assumptions, a quick glance of the plots reveals some slight homoskedasticity, with points fanning out as x-axis increases. These observations are presumably independent and random and residuals look fairly normal in distribution. Using a Breusch-Pagan test to formally assess homoskedsticity, p-value returned is very small, so we can reject the null hypothesis of meeting homoskedasticity. After adjusting for lack of homoskedasticity with robust standard errors, we can get a better representation of our data, with larger standard errors for each variable/interaction. This is effectively a way to compensate for the dataset breaking assumptions and not being ideal. Some of these hits are pretty significant, with 6 Rooms taking a 100,000 increase and 5 Rooms taking a 180,000 increase!! We take a pretty hefty penalty for breaking the assumptions here. 
 
```{R}

summary(fit)

fit %>% ggplot(aes(x=lol$BuildingArea_c,y=lol$Price))+geom_point(aes(x=lol$BuildingArea_c,y=lol$Price)) + coord_cartesian(xlim=c(0,1000))

residuals <- lm(Price ~ BuildingArea_c * Rooms, data = lol)$residuals
ggplot()+geom_histogram(aes(residuals),bins=100)

bptest(fit)

summary(fit)$coef[,1:2]
coeftest(fit, vcov = vcovHC(fit))[,1:2]

```
 
 After computing for bootstrapped standard errors, there seems to be less standard error compared to both the regular and robust standard errors in the previous tests. All values decrease by anywhere from 100 (Building Area) to 105,000 (5 Rooms! Super impressive). This means that with bootstrapping and randomization, there is a tighter spread of these interactions, and are seemingly more reflective of observed data (assuming this data is ideal). 
 
```{R}

bootstrap_bill <-sample_frac(lol, replace=T)

samp_distn<-replicate(5000, {
  bootstrap_bill <-sample_frac(lol, replace=T)
  fitted<-lm(Price ~ BuildingArea_c * Rooms, data = bootstrap_bill)
  coef(fitted)
})

samp_distn %>% t %>% as.data.frame %>% summarize_all(sd)

```
 
 First, I establish a binary variable from the categorical I've been using for experimentation this far, Rooms. I split it right down the middle, with 1-3 Rooms being the 'Normal' category and 4-6 being the 'Family' category. A logistical regression is then run to observe how the new Size variable is affected by a house's Price and BuildingArea. Because this is a logodds operation, all estimates need to be exponentiated before they can be used to determine the odds of a home being Normal. For example, a home with a price 100,000 dollars above the average with a building area 90 square meters above the average has a log odds of -0.938 + 0.041 + 2.141 = 1.244. Exponentiating this gives an odds value of 3.469, which translates to a probability of (3.469)/(1+3.469). This means under these conditions, the probability of being a 'Family' sized house is 77.6%. 
 
```{R}

lol %>% mutate(Size = case_when(Rooms > 3 ~ "Family", Rooms <= 3 ~ "Normal")) -> lol
lol <- lol%>%mutate(y=ifelse(Size=="Family",1,0)) -> lol

newfit <- glm(y ~ Price_c+BuildingArea_c, data = lol, family = binomial(link="logit"))
coeftest(newfit)


```
 
 After adding a new column to determine the probability, we can use this probability to sort the expected house sizes based on their characteristics. If R computes that a house with lots of space and a high price has a probability greater than 0.5, this means it is closer to the Family category, and it will sort it as such. A confusion table was then established to compare the efficacy of this model with actual results. The accuracy of the report is the amount of correctly classified cases over the total number of cases, which is the 1646 Family and 6806 Normal out of the total 8874 cases, for a value of 0.952. Sensitivity (True Positive Rate, TPR) is a little weaker, with only 1646 correctly classified Family houses out of a total of 2798, for a percentage of 0.588. Specificity (True Negative Rate, TNR) is better, with 5654 correctly classified Normal out of 6076, for a percentage of 0.931. The Precision (Positive Predicted Value, PPV), the number of houses classified as Family that actually are is 1646/2068, or 0.796. 
 
```{R}

lol$prob <- predict(newfit,type="response")
lol$predicted <- ifelse(lol$prob>.5,"Family","Normal")
table(truth=lol$Size, prediction=lol$predicted) %>% addmargins


```
 
 A plot comparing the density of the log-odds sorted by Size is provided below. Essentially, x=0 is the indicator for what is positive (a Family house) and what is negative (a Normal house). Any overlap that occurs between the two variables is then categorized as a false positive or a false negative depending on which side of the x=0 line it falls. If this data had been perfect, both curves would exist on either side of this line with no overlap, indicating every single house was properly identified based on the logical regression model. 
 
```{R}

odds2prob<-function(odds){odds/(1+odds)}

probs<-predict(newfit,type="response")
lol$logit<-predict(newfit,type="link")

lol %>%ggplot()+geom_density(aes(logit,color=Size,fill=Size), alpha=.4)+
  theme(legend.position=c(.85,.85))+geom_vline(xintercept=0)+xlab("logit (log-odds)")+
  geom_rug(aes(logit,color=Size))+
  geom_text(x=-5,y=.07,label="TN = 431")+
  geom_text(x=-1.75,y=.008,label="FN = 19")+
  geom_text(x=1,y=.006,label="FP = 13")+
  geom_text(x=5,y=.04,label="TP = 220") + coord_cartesian(xlim=c(-7,20))

```
 
 An ROC was made for this dataset observing the amount of true positives that can be assume while limiting false positives. The AUC was computed using a package and the AUC for this model returned was 0.893. This means that the model itself, basing whether a house is Family size or Normal size based on its price or building area is actually a really good, predictive model! Most of the observations measured using these variables can accurately predict whether a house is Family size or not. 
 
```{R}

ROCplot <-ggplot(lol)+geom_roc(aes(d=y,m=prob), n.cuts=0)+  geom_segment(aes(x=0,xend=1,y=0,yend=1),lty=2) 
ROCplot
calc_auc(ROCplot)

```
 
 For this next portion, I spent time further cleaning up my dataset so things wouldn't get weird. I adjusted things into 'character' data types, such as neighborhood names and regions. I also did this with things like bathroom number and garage size (car), as they seemed more closely aligned with bins in a categorical variable than a real numeric value anyways. Same thing as earlier: run a logical regression, get interactions for log odds, determine probability of being above 0.5 the log odds of a Family size house, and create a confusion matrix from this. These values are: Accuracy (2086+5555/8874 = 0.861), Sensitivity (2086+2798 = 0.746), Specificity (5555/6076 = 0.914), and Precision (2086/2607 = 0.800). Overall, still a pretty good model. Loses some accuracy and sensitivity, but other values are better. This means the model is now predicting less correct Family houses, and actually falsely categorizing Normal houses as Family houses as well. That being said, it guesses Normal houses pretty well!
 
```{R}

lol$Bathroom <- as.factor(as.integer(lol$Bathroom))
lol$Car <- as.factor(as.integer(lol$Car))

newestfit <- glm(y ~ Price_c+BuildingArea_c+Suburb+Bathroom+Car+CouncilArea+Regionname, data = lol, family = binomial(link="logit"))
coeftest(newestfit)

lol$prob <- predict(newestfit,type="response")
lol$predicted <- ifelse(lol$prob>.5,"Family","Normal")
table(truth=lol$Size, prediction=lol$predicted) %>% addmargins

prob <- predict(newestfit,type="response")

```
 
 First order of business: setting up the class_diag function and double checking everything is correct. Working well! AUC of 0.924. 
 
```{R}

class_diag<-function(probs,truth){
  
  tab<-table(factor(probs>.5,levels=c("FALSE","TRUE")),truth)
  acc=sum(diag(tab))/sum(tab)
  sens=tab[2,2]/colSums(tab)[2]
  spec=tab[1,1]/colSums(tab)[1]
  ppv=tab[2,2]/rowSums(tab)[2]

  if(is.numeric(truth)==FALSE & is.logical(truth)==FALSE) truth<-as.numeric(truth)-1
  
  ord<-order(probs, decreasing=TRUE)
  probs <- probs[ord]; truth <- truth[ord]
  
  TPR=cumsum(truth)/max(1,sum(truth)) 
  FPR=cumsum(!truth)/max(1,sum(!truth))
  
  dup<-c(probs[-1]>=probs[-length(probs)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )

  data.frame(acc,sens,spec,ppv,auc)
}

class_diag(lol$prob,lol$y)
auc(lol$y,lol$prob)

```
 
 Things here start to kind of fall apart :/ I created a new dataset to include only the variables of interest and attempted to run the 10-fold CV, but it kept returning an error message that was indiscernible. I am fairly certain everything was setup correct to run the loop. This essentially ruined the rest of this portion :/ In theory, we would be looking at the AUC in this portion and comparing it to the AUC obtained from the model before this. Likely, this AUC would be smaller, suggesting that there was overfitting in the original model. 
 
```{R}

lol %>% select(1,5,6,10,11,12,13,14) -> lol2
lol2 %>% na.omit()
lol2$Size <- as.factor(as.character(lol2$Size))

fit <- glm(Size~(.),data=lol2,family=binomial)
probs <- predict(fit, type = "response")
class_diag(probs, lol2$Size)

table(pred=probs >.5, truth=lol2$Size)
  
set.seed(1234)
k=10

data1 <- lol2[sample(nrow(lol2)),]
folds <- cut(1:nrow(lol2), breaks = k, labels = F)

#diags <- NULL
#for(i in 1:k){
  #train <- data1[folds!=i,]
  #test <- data1[folds==i,]
  #truth <- test$Size
  #fit<-glm(Size~.,data=train,family="binomial")
  #probs <- predict(fit, newdata = test, type="response")
  #diags <- rbind(diags, class_diag(probs, truth))}

#summarize_all(diags,mean)

```
 
 Same thing here: there was an error in the observations/rows even though there were no disagreements in the dataset itself, so LASSO wouldn't work. In theory, we would be looking for the non-zero estimates returned and would use those to make another regression containing only variables of interest that really explain the Size outcome. Recomputing the 10-fold CV would show more resilience to CV compared to the last model, again showing that the full model was probaly overfitting. Sorry this didn't work out :/ 
 
```{R}

library(glmnet)
set.seed(1234)

ahh <- glm(Size~(.),data=lol2,family=binomial)

predictor <- model.matrix(fit)[,-1]
response <- as.matrix(lol$Size)

#cv.lasso1 <- cv.glmnet(x=predictor,y=response, family="binomial")
#lasso1 <- glmnet(x=predictor,y=response, family="binomial", alpha =1, lambda=cv.lasso1$lambda.1se)
#coef(lasso1)

#lol3 <- cbind(as.data.frame(predictor), as.data.frame(response))

#fit_reg <- glm(
           #data=lol3, family = "binomial")
#summary(fit_reg)
#prob_reg <- predict(fit_reg, type="response")

#table(predict=prob_reg>0.5, truth=lol$Size)
#class_diag(prob_reg, lol$Size)


```
 
 